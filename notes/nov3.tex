\documentclass[12pt, AMS Euler]{article}
\textheight=9.25in \textwidth=7in \topmargin=-.75in
 \oddsidemargin=-0.25in
\evensidemargin=-0.25in
\usepackage{url}  % The bib file uses this
\usepackage{graphicx} %to import pictures
\usepackage{amsmath, amssymb}
\usepackage{theorem, concrete, multicol, color}
\usepackage{listings}

\setlength{\intextsep}{5mm} \setlength{\textfloatsep}{5mm}
\setlength{\floatsep}{5mm}
\setlength{\parindent}{0em} % new paragraphs are not indented


%%%%  SHORTCUT COMMANDS  %%%%
\newcommand{\ds}{\displaystyle}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\arc}{\rightarrow}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\blank}{\underline{\hspace{0.33in}}}
\newcommand{\qand}{\quad and \quad}
\newcommand{\stirling}[2]{\genfrac{\{}{\}}{0pt}{}{#1}{#2}}
\newcommand{\dydx}{\ds \frac{d y}{d x}}
\newcommand{\ddx}{\ds \frac{d}{d x}}
\newcommand{\dvdx}{\ds \frac{d v}{d x}} 

%%%%  footnote style %%%%

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\pagestyle{empty}

\begin{document}

\begin{flushright}
Chandler Justice - A02313187
\end{flushright}

\textbf{November 2, 2023}


Suppose we want to define the smallest Eigenvalue:

\textit{Input}: $A, v_0, tol, matIter$\\

\begin{listings}
	while(error $<$ tol \&\& iter $<$ matIter)\\
	\{
		v1 = 1 / magnitude(y) * y\\
		w = A * v1\\
		$\lambda_1$ = $v_1^t * w$\\

		error = abs($\lambda_0 - \lambda_1$)\\
		iter++;\\
		$\lambda_0 = \lambda_1$\\

		$y = v_1$\\

	\}\\
	return $\lambda_1$, error;
\end{listings}

\textbf{Finding the smallest Eigenvalue:} If $\lambda_1$ is the largest eigenvector of $A$, then $\frac{1}{\lambda_1}$ is the smallest eigenvalue of $A^{-1}$. If $\lambda_n$ is the smallest eigenvalue of $A$. the $\frac{1}{\eigenvalue_n}$ is the largest eigenvalue of $A^{-1}$\\

We have the means to compute approximations of $\lambda_1$ and $\lambda_n$
\begin{itemize}
	\item $\lambda_1$: Use the power method
	\item $\lambda_n$: Use the inverse power method
\end{itemize}\\

\textbf{properties of eigenvalues}:\\

if $(\lambda, v)$ an eigen pair, $v \neq 0$. this means $A v = \lambda v$. Then for $\mu \in \R$, ($Av - \mu I v) = (A - \mu I)v = \lambda v - \mu I v = (\lambda - \mu) v$\\
$\rightarrow (\lambda - \mu)$ is an eigenvalue of $(A - \mu I)$.\\
So, $(A - \mu I) v = (\lambda - \mu) v$\\

The idea is choose $\mu$ close our eigenvalue. We can use inverse iteration to derive an approximation of $\lambda - \mu$ and then add $\mu$ back to get $\lambda$.\\

\textbf{Iteratively finding eigenvalues}:\\

$v_0 = a_1 v_1 + a_2 v_2 + ... + a_n v_n$\\

$Av_0 = a_1(\lambda_1 v_1) + a_2 (\lambda_2 v_2) + ... + a_n (\lambda_n v_n)$\\

where $\lambda_1 /> \lambda_2 \geq ... \geq \lambda_{n-1} /> 0$\\


\textit{Properties}\\

1. $\frac{Av_k}{||v_k||} \rightarrow v_1$\\

2. $\frac{v_k^t A_{v_k}}{v_k^t v_k}$\\


3. If $\lambda$ is an eigenvalue of $A$, the $\frac{1}{\lambda}$ is an eigenvalue of $A^{-1}$ \\

4. $A \vec{v} = \lambda \vec{v} \rightarrow A \vec{v} - \mu v = (\lambda - \mu) v \rightarrow (A - \mu I) v$.\\
\quad If $\lambda$ is an eigenvalue of $A$, then $\lambda - \mu$ is an eigenvalue of $A \mu I$\\

Using this approach allows us to utilize parallelism. This means we can use a tool like \textit{openMP} to easily parallelize code written in C, C++, fortran, etc\\

\textbf{Lamczos Algorithm}\\

$Ax \rightarrow b$\\
$
\begin{bmatrix}
	\sum_{j=i}^n a_{i,j} x_j\\
	\sum_{j=i}^n a_{i,j} x_j\\
	...\\
\end{bmatrix}
$
=
$
\begin{bmatrix}
	b_{1}\\
	b_{2}\\
	...\\
	b_{n}\\
\end{bmatrix}
$
=\\

$ x_1
\begin{bmatrix}
	a_{1,1}\\
	a_{1,2}\\
	...\\
	a_{1,n}\\
\end{bmatrix}
+ x_2
\begin{bmatrix}
	a_{2,1}\\
	a_{2,2}\\
	...\\
	a_{2,n}\\
\end{bmatrix}
+ ... + x_n
\begin{bmatrix}
	a_{n,1}\\
	a_{n,2}\\
	...
	a_{n,n}\\
\end{bmatrix}
$
\begin{listings}

for(int i = 0; i < n; i++)\{\\
	double sum = 0.0;\\
	for(int j = 0; j < n; j++)\{\\
		sum += a[i][j] * x[j];\\
	\}\\
	b[i] = sum;\\
	\}\\
\end{listings}

\noindent \underline{\hspace{3in}}\\


\end{document}

